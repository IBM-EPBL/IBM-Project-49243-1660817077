{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Msftedit 5.41.21.2510;}\viewkind4\uc1\pard\sa200\sl276\slmult1\lang9\f0\fs22 from google.colab import drive\par
drive.mount('/content/drive')\par
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).\par
from tensorflow.keras.layers import Dense, Flatten, Input\par
from tensorflow.keras.models import Model\par
from tensorflow.keras.preprocessing import image\par
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\par
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\par
from glob import glob\par
import numpy as np\par
import matplotlib.pyplot as plt\par
imageSize = [224, 224]\par
\par
trainPath = r"/content/drive/MyDrive/dataset1/body/training"\par
\par
testPath = r"/content/drive/MyDrive/dataset1/body/validation"\par
# adding preprocessing layers to the front of vgg\par
\par
vgg = VGG16(input_shape=imageSize + [3], weights='imagenet',include_top=False)\par
Downloading data from {\field{\*\fldinst{HYPERLINK "https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"}}{\fldrslt{\ul\cf1 https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5}}}\f0\fs22\par
58889256/58889256 [==============================] - 0s 0us/step\par
# don't train existing weights\par
for layer in vgg.layers:\par
  layer.trainable = False\par
# our layers - you can add more if you want\par
x = Flatten()(vgg.output)\par
prediction = Dense(3, activation='softmax')(x)\par
# create a model object\par
model = Model(inputs=vgg.input, outputs=prediction)\par
# view the structure of the model\par
model.summary()\par
Model: "model"\par
_________________________________________________________________\par
 Layer (type)                Output Shape              Param #   \par
=================================================================\par
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \par
                                                                 \par
 block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \par
                                                                 \par
 block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \par
                                                                 \par
 block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \par
                                                                 \par
 block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \par
                                                                 \par
 block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \par
                                                                 \par
 block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \par
                                                                 \par
 block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \par
                                                                 \par
 block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \par
                                                                 \par
 block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \par
                                                                 \par
 block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \par
                                                                 \par
 block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \par
                                                                 \par
 block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \par
                                                                 \par
 block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \par
                                                                 \par
 block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \par
                                                                 \par
 block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \par
                                                                 \par
 block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \par
                                                                 \par
 block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \par
                                                                 \par
 block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \par
                                                                 \par
 flatten (Flatten)           (None, 25088)             0         \par
                                                                 \par
 dense (Dense)               (None, 3)                 75267     \par
                                                                 \par
=================================================================\par
Total params: 14,789,955\par
Trainable params: 75,267\par
Non-trainable params: 14,714,688\par
_________________________________________________________________\par
# tell the model what cost and optimization method to use\par
model.compile(\par
  loss='categorical_crossentropy',\par
  optimizer='adam',\par
  metrics=['accuracy']\par
)\par
train_datagen = ImageDataGenerator(rescale = 1./255,\par
                                   shear_range = 0.2,\par
                                   zoom_range = 0.2,\par
                                   horizontal_flip = True)\par
\par
test_datagen = ImageDataGenerator(rescale = 1./255)\par
training_set = train_datagen.flow_from_directory(trainPath,\par
                                                 target_size = (224, 224),\par
                                                 batch_size = 10,\par
                                                 class_mode = 'categorical')\par
\par
test_set = test_datagen.flow_from_directory(testPath,\par
                                            target_size = (224, 224),\par
                                            batch_size = 10,\par
                                            class_mode = 'categorical')\par
Found 979 images belonging to 3 classes.\par
Found 171 images belonging to 3 classes.\par
import sys\par
# fit the model\par
r = model.fit_generator(\par
  training_set,\par
  validation_data=test_set,\par
  epochs=10,\par
  steps_per_epoch=979//10,\par
  validation_steps=171//10)\par
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\par
  \par
Epoch 1/10\par
97/97 [==============================] - 596s 6s/step - loss: 1.2100 - accuracy: 0.5253 - val_loss: 1.0260 - val_accuracy: 0.6294\par
Epoch 2/10\par
97/97 [==============================] - 588s 6s/step - loss: 0.7407 - accuracy: 0.7110 - val_loss: 0.9575 - val_accuracy: 0.6706\par
Epoch 3/10\par
97/97 [==============================] - 589s 6s/step - loss: 0.6132 - accuracy: 0.7534 - val_loss: 0.8389 - val_accuracy: 0.7000\par
Epoch 4/10\par
97/97 [==============================] - 587s 6s/step - loss: 0.4645 - accuracy: 0.8390 - val_loss: 1.3165 - val_accuracy: 0.6412\par
Epoch 5/10\par
97/97 [==============================] - 589s 6s/step - loss: 0.4019 - accuracy: 0.8514 - val_loss: 1.0316 - val_accuracy: 0.6529\par
Epoch 6/10\par
97/97 [==============================] - 588s 6s/step - loss: 0.2716 - accuracy: 0.8999 - val_loss: 1.0882 - val_accuracy: 0.6529\par
Epoch 7/10\par
97/97 [==============================] - 594s 6s/step - loss: 0.2722 - accuracy: 0.9071 - val_loss: 1.0481 - val_accuracy: 0.6765\par
Epoch 8/10\par
97/97 [==============================] - 592s 6s/step - loss: 0.2265 - accuracy: 0.9289 - val_loss: 1.3173 - val_accuracy: 0.6059\par
Epoch 9/10\par
97/97 [==============================] - 593s 6s/step - loss: 0.2981 - accuracy: 0.8751 - val_loss: 1.1330 - val_accuracy: 0.6941\par
Epoch 10/10\par
97/97 [==============================] - 592s 6s/step - loss: 0.2247 - accuracy: 0.9123 - val_loss: 1.5393 - val_accuracy: 0.5706\par
#save the model\par
model.save('body.h5')\par
#import load_model class for loading h5 file\par
from tensorflow.keras.models import load_model\par
#import image class to process the images\par
from tensorflow.keras.preprocessing import image\par
from tensorflow.keras.applications.inception_v3 import preprocess_input\par
import numpy as np\par
#load one random image from local system\par
img=image.load_img(r'/content/drive/MyDrive/dataset1/body/training/00-front/0002.JPEG',target_size=(224,224))\par
#convert image to array format\par
x=image.img_to_array(img)\par
import numpy as np\par
x=np.expand_dims(x,axis=0)\par
img_data=preprocess_input(x)\par
img_data.shape\par
(1, 224, 224, 3)\par
img_data.shape\par
(1, 224, 224, 3)\par
model.predict(img_data)\par
1/1 [==============================] - 1s 732ms/step\par
array([[9.9837422e-01, 1.6256354e-03, 7.2354190e-08]], dtype=float32)\par
output=np.argmax(model.predict(img_data), axis=1)\par
output\par
1/1 [==============================] - 1s 535ms/step\par
array([0])\par
imageSize = [224, 224]\par
\par
trainPath = r"/content/drive/MyDrive/dataset1/level/training"\par
\par
testPath = r"/content/drive/MyDrive/dataset1/level/validation"\par
vgg1 = VGG16(input_shape=imageSize + [3], weights='imagenet',include_top=False)\par
for layer in vgg1.layers:\par
  layer.trainable = False\par
# our layers - you can add more if you want\par
x = Flatten()(vgg1.output)\par
prediction = Dense(3, activation='softmax')(x)\par
# create a model object\par
model1 = Model(inputs=vgg1.input, outputs=prediction)\par
# tell the model what cost and optimization method to use\par
model1.compile(\par
  loss='categorical_crossentropy',\par
  optimizer='adam',\par
  metrics=['accuracy']\par
)\par
train_datagen = ImageDataGenerator(rescale = 1./255,\par
                                   shear_range = 0.2,\par
                                   zoom_range = 0.2,\par
                                   horizontal_flip = True)\par
\par
test_datagen = ImageDataGenerator(rescale = 1./255)\par
training_set = train_datagen.flow_from_directory(trainPath,\par
                                                 target_size = (224, 224),\par
                                                 batch_size = 10,\par
                                                 class_mode = 'categorical')\par
\par
test_set = test_datagen.flow_from_directory(testPath,\par
                                            target_size = (224, 224),\par
                                            batch_size = 10,\par
                                            class_mode = 'categorical')\par
Found 979 images belonging to 3 classes.\par
Found 171 images belonging to 3 classes.\par
r = model1.fit_generator(\par
  training_set,\par
  validation_data=test_set,\par
  epochs=10,\par
  steps_per_epoch=979//10,\par
  validation_steps=171//10)\par
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\par
  \par
Epoch 1/10\par
97/97 [==============================] - 600s 6s/step - loss: 0.9744 - accuracy: 0.6336 - val_loss: 0.9600 - val_accuracy: 0.6353\par
Epoch 2/10\par
97/97 [==============================] - 594s 6s/step - loss: 0.7424 - accuracy: 0.7069 - val_loss: 0.9975 - val_accuracy: 0.6353\par
Epoch 3/10\par
97/97 [==============================] - 595s 6s/step - loss: 0.5972 - accuracy: 0.7812 - val_loss: 1.1393 - val_accuracy: 0.6176\par
Epoch 4/10\par
97/97 [==============================] - 594s 6s/step - loss: 0.4651 - accuracy: 0.8122 - val_loss: 1.1309 - val_accuracy: 0.5941\par
Epoch 5/10\par
97/97 [==============================] - 595s 6s/step - loss: 0.3979 - accuracy: 0.8349 - val_loss: 1.1914 - val_accuracy: 0.5706\par
Epoch 6/10\par
97/97 [==============================] - 595s 6s/step - loss: 0.3280 - accuracy: 0.8689 - val_loss: 1.2503 - val_accuracy: 0.5824\par
Epoch 7/10\par
97/97 [==============================] - 596s 6s/step - loss: 0.3338 - accuracy: 0.8741 - val_loss: 1.0894 - val_accuracy: 0.6176\par
Epoch 8/10\par
97/97 [==============================] - 599s 6s/step - loss: 0.2654 - accuracy: 0.9123 - val_loss: 1.1027 - val_accuracy: 0.6471\par
Epoch 9/10\par
97/97 [==============================] - 595s 6s/step - loss: 0.2324 - accuracy: 0.9143 - val_loss: 1.2071 - val_accuracy: 0.6118\par
Epoch 10/10\par
97/97 [==============================] - 596s 6s/step - loss: 0.1750 - accuracy: 0.9381 - val_loss: 1.1278 - val_accuracy: 0.6353\par
#save the model\par
model.save('level.h5')\par
}
 